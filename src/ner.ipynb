{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "with open('training_data/ner/ner_data.json', 'r') as f:\n",
    "    train_sentences = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "58d5206a229b4b9d926b81ed4eac8d34"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zineddine\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Zineddine\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Zineddine\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 2/20 : < :, Epoch 1/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification, BertTokenizerFast, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "trained_ner_location = 'trained_models/ner'\n",
    "\n",
    "ner_already_trained = os.path.exists(trained_ner_location)\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = BertForTokenClassification.from_pretrained(trained_ner_location) if ner_already_trained else BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define a mapping from label names to indices\n",
    "label_to_index = {\"O\": 0, \"setting_name\": 1, \"state\": 2}\n",
    "index_to_label = {v: k for k, v in label_to_index.items()}\n",
    "\n",
    "if not ner_already_trained:\n",
    "    # Function to encode the examples\n",
    "    def encode_example(sentence):\n",
    "        inputs = tokenizer(sentence['text'], is_split_into_words=True, padding='max_length', truncation=True, max_length=128)\n",
    "        labels = [-100 if token_id==tokenizer.pad_token_id else label_to_index[label] for token_id, label in zip(inputs['input_ids'], sentence['ner'])]  \n",
    "        labels += [-100] * (128 - len(labels))  # pad labels to the max length\n",
    "        inputs['labels'] = labels\n",
    "        return inputs\n",
    "\n",
    "\n",
    "    # Encode all examples\n",
    "    train_encodings = [encode_example(s) for s in train_sentences]\n",
    "    train_encodings = [ {k: torch.tensor(v) for k, v in enc.items()} for enc in train_encodings]\n",
    "\n",
    "    # Define the training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=20,\n",
    "        per_device_train_batch_size=64,\n",
    "        weight_decay=0.01,\n",
    "        seed=42,  # for reproducibility\n",
    "        logging_steps=100,  # log loss every 100 steps\n",
    "        # logging_dir='./logs',\n",
    "    )\n",
    "\n",
    "    # Define the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_encodings,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    model.save_pretrained(trained_ner_location)\n",
    "\n",
    "# Function to extract settings and states\n",
    "def extract_settings_and_states(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=2)\n",
    "\n",
    "    predicted_labels = [index_to_label[p] for p in predictions[0].tolist()]\n",
    "\n",
    "    # Match up the original tokens with their predicted labels\n",
    "    original_tokens = tokenizer.tokenize(sentence)\n",
    "    aligned_labels = []\n",
    "    current_tokens = []\n",
    "    current_label = None\n",
    "\n",
    "    for token, label in zip(original_tokens, predicted_labels):\n",
    "        # Handle subwords\n",
    "        if token.startswith(\"##\"):\n",
    "            if current_tokens:  # ensure current_tokens is not empty\n",
    "                current_tokens[-1] += token[2:]\n",
    "        else:                \n",
    "            # If the label is not 'O', add the token to current_tokens\n",
    "            # and update current_label\n",
    "            if label != 'O':\n",
    "                current_tokens.append(token)\n",
    "                current_label = label\n",
    "            else:\n",
    "                # If current_label is not None, add the assembled entity and its label\n",
    "                # to aligned_labels\n",
    "                if current_label:\n",
    "                    aligned_labels.append([' '.join(current_tokens), current_label])\n",
    "                    current_tokens = []\n",
    "                    current_label = None\n",
    "    \n",
    "    # Handle the last entity if its label is not 'O'\n",
    "    if current_label:\n",
    "        aligned_labels.append([' '.join(current_tokens), current_label])\n",
    "\n",
    "    return aligned_labels\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted entities: [['publish', 'state'], ['profile', 'setting_name']]\n",
      "Predicted entities: [['disable the connect with me', 'setting_name']]\n",
      "Predicted entities: [['search', 'setting_name']]\n",
      "Predicted entities: [['logged in', 'state']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted entities:\", extract_settings_and_states(\"How can I publish my profile?\"))\n",
    "print(\"Predicted entities:\", extract_settings_and_states(\"How do I disable the connect with me setting?\"))\n",
    "print(\"Predicted entities:\", extract_settings_and_states(\"How do i make myself visible in the search?\"))\n",
    "print(\"Predicted entities:\", extract_settings_and_states(\"Can i make myself only findable to logged in users?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
