{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "with open('training_data/ner/ner_data.json', 'r') as f:\n",
    "    train_sentences = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification, BertTokenizerFast, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "trained_ner_location = 'trained_models/ner'\n",
    "\n",
    "ner_already_trained = os.path.exists(trained_ner_location)\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = BertForTokenClassification.from_pretrained(trained_ner_location) if ner_already_trained else BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define a mapping from label names to indices\n",
    "label_to_index = {\"O\": 0, \"setting_name\": 1, \"state\": 2}\n",
    "index_to_label = {v: k for k, v in label_to_index.items()}\n",
    "\n",
    "if not ner_already_trained:\n",
    "    # Function to encode the examples\n",
    "    def encode_example(sentence):\n",
    "        inputs = tokenizer(sentence['text'], is_split_into_words=True, padding='max_length', truncation=True, max_length=128)\n",
    "        labels = [-100 if token_id==tokenizer.pad_token_id else label_to_index[label] for token_id, label in zip(inputs['input_ids'], sentence['ner'])]  \n",
    "        labels += [-100] * (128 - len(labels))  # pad labels to the max length\n",
    "        inputs['labels'] = labels\n",
    "        return inputs\n",
    "\n",
    "\n",
    "    # Encode all examples\n",
    "    train_encodings = [encode_example(s) for s in train_sentences]\n",
    "    train_encodings = [ {k: torch.tensor(v) for k, v in enc.items()} for enc in train_encodings]\n",
    "\n",
    "    # Define the training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=20,\n",
    "        per_device_train_batch_size=64,\n",
    "        weight_decay=0.01,\n",
    "        seed=42,  # for reproducibility\n",
    "        logging_steps=100,  # log loss every 100 steps\n",
    "        # logging_dir='./logs',\n",
    "    )\n",
    "\n",
    "    # Define the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_encodings,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    model.save_pretrained(trained_ner_location)\n",
    "\n",
    "# Function to extract settings and states\n",
    "def extract_settings_and_states(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=2)\n",
    "\n",
    "    predicted_labels = [index_to_label[p] for p in predictions[0].tolist()]\n",
    "\n",
    "    # Match up the original tokens with their predicted labels\n",
    "    original_tokens = tokenizer.tokenize(sentence)\n",
    "    aligned_labels = []\n",
    "    current_tokens = []\n",
    "    current_label = None\n",
    "\n",
    "    for token, label in zip(original_tokens, predicted_labels):\n",
    "        # Handle subwords\n",
    "        if token.startswith(\"##\"):\n",
    "            if current_tokens:  # ensure current_tokens is not empty\n",
    "                current_tokens[-1] += token[2:]\n",
    "        else:                \n",
    "            # If the label is not 'O', add the token to current_tokens\n",
    "            # and update current_label\n",
    "            if label != 'O':\n",
    "                current_tokens.append(token)\n",
    "                current_label = label\n",
    "            else:\n",
    "                # If current_label is not None, add the assembled entity and its label\n",
    "                # to aligned_labels\n",
    "                if current_label:\n",
    "                    aligned_labels.append([' '.join(current_tokens), current_label])\n",
    "                    current_tokens = []\n",
    "                    current_label = None\n",
    "    \n",
    "    # Handle the last entity if its label is not 'O'\n",
    "    if current_label:\n",
    "        aligned_labels.append([' '.join(current_tokens), current_label])\n",
    "\n",
    "    return aligned_labels\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicted entities:\", extract_settings_and_states(\"How can I publish my profile?\"))\n",
    "print(\"Predicted entities:\", extract_settings_and_states(\"How do I disable the connect with me setting?\"))\n",
    "print(\"Predicted entities:\", extract_settings_and_states(\"How do i make myself visible in the search?\"))\n",
    "print(\"Predicted entities:\", extract_settings_and_states(\"Can i make myself only findable to logged in users?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
