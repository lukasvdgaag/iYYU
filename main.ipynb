{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup, BertConfig\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import json\n",
    "\n",
    "# Load the data from JSON file\n",
    "with open(\"sentiment_analysis.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the training data\n",
    "train_data = []\n",
    "# sub_labels = {}\n",
    "# for top_level_label, sub_level_labels in data.items():\n",
    "#     for sub_level_label_data in sub_level_labels:\n",
    "#         text = sub_level_label_data[\"text\"]\n",
    "#         sub_level_label = sub_level_label_data[\"sub_level_label\"]\n",
    "#         train_data.append((text, top_level_label, sub_level_label))\n",
    "#         if top_level_label not in sub_labels:\n",
    "#             sub_labels[top_level_label] = []\n",
    "#         sub_labels[top_level_label].append(sub_level_label)\n",
    "\n",
    "intent_subintent_map = {}\n",
    "subintent_labels = set()\n",
    "for intent, examples in data.items():\n",
    "    for example in examples:\n",
    "        subintent_label = example['sub_level_label']\n",
    "        subintent_labels.add(subintent_label)\n",
    "        intent_subintent_map[(intent, subintent_label)] = len(subintent_labels) - 1\n",
    "\n",
    "print(intent_subintent_map)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# convert the input data to suitable input format for the BERT model\n",
    "inputs = []\n",
    "labels = []\n",
    "for intent, examples in data.items():\n",
    "    for example in examples:\n",
    "        text = example['text']\n",
    "        subintent_label = example['sub_level_label']\n",
    "        input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "        inputs.append(input_ids)\n",
    "        labels.append(intent_subintent_map[(intent, subintent_label)])\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# load the BERT model configuration and tokenizer\n",
    "model_config = BertConfig.from_pretrained('bert-base-uncased', num_labels=len(subintent_labels))\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# create the BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=model_config)\n",
    "\n",
    "# set up the optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(inputs) * num_epochs)\n",
    "\n",
    "# convert the input data to PyTorch tensors\n",
    "inputs = torch.tensor(inputs)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# create a PyTorch dataset and data loader\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=24)\n",
    "\n",
    "# train the BERT model\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    for batch in dataloader:\n",
    "        # get the input and label data for this batch\n",
    "        batch_inputs, batch_labels = batch\n",
    "\n",
    "        # zero out the gradients from the previous batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass through the BERT model and compute the loss\n",
    "        outputs = model(batch_inputs, labels=batch_labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # perform backward pass and update the parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "# Define the mapping between top-level labels and integers\n",
    "# top_level_label_map = {label: i for i, label in enumerate(set([data[1] for data in train_data]))}\n",
    "# print(top_level_label_map)\n",
    "\n",
    "# Define the mapping between sub-level labels and integers\n",
    "# sub_level_label_map = {sub_label: i for i, sub_label in enumerate(set([sub_label for sub_labels_list in sub_labels.values() for sub_label in sub_labels_list]))}\n",
    "# print(sub_level_label_map)\n",
    "\n",
    "# Convert the training data labels to integers using the label_map and sub_label_map\n",
    "# A tensor is a multi-dimensional array that looks like a numpy array, it's used for neural networks\n",
    "# top_level_labels = torch.tensor([top_level_label_map[data[1]] for data in train_data])\n",
    "# sub_level_labels = torch.tensor([sub_level_label_map[sub_label] for data in train_data for sub_label in sub_labels[data[1]]])\n",
    "\n",
    "# # Load the pre-trained BERT model and tokenizer\n",
    "# model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(top_level_label_map))\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# # Tokenize the training data and convert to tensors\n",
    "# inputs = tokenizer.batch_encode_plus([data[0] for data in train_data], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# # Fine-tune the model on the training data\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# for epoch in range(10):\n",
    "#     optimizer.zero_grad()\n",
    "#     outputs = model(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], labels=top_level_labels)\n",
    "#     loss = outputs.loss\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "\n",
    "#     print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "#     # Evaluate the model on the training data\n",
    "#     predictions = outputs.logits.argmax(axis=1)\n",
    "#     accuracy = (predictions == top_level_labels).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "\n",
    "import torch\n",
    "\n",
    "def predict_intent_subintent(input_text):\n",
    "    # Preprocess the input text\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    input_tokens = tokenizer(input_text, padding=True, truncation=True, return_tensors='pt')\n",
    "    input_ids = input_tokens['input_ids'].to(device)\n",
    "    attention_mask = input_tokens['attention_mask'].to(device)\n",
    "\n",
    "    # Wrap the input in a dictionary\n",
    "    inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Pass the input to the model and get the output logits\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Get the index of the highest scoring logits as the predicted label\n",
    "    predicted_label_index = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    # Map the predicted label to the corresponding intent and sub-intent using the intent_subintent_map\n",
    "    predicted_intent, predicted_subintent = intent_subintent_map[predicted_label_index]\n",
    "\n",
    "    # Return the predicted intent and sub-intent\n",
    "    return predicted_intent, predicted_subintent\n",
    "\n",
    "\n",
    "# def predict_intent(text):\n",
    "\n",
    "#     top_level_inputs = tokenizer.encode_plus(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "#     top_level_outputs = model(top_level_inputs[\"input_ids\"], attention_mask=top_level_inputs[\"attention_mask\"])\n",
    "#     # top_level_predicted_labels = torch.argsort(top_level_outputs.logits, descending=True).tolist()[0]\n",
    "#     top_level_predicted_label = torch.argmax(top_level_outputs.logits).item()\n",
    "#     top_level_predicted_intent = [k for k, v in top_level_label_map.items() if v == top_level_predicted_label]\n",
    "    \n",
    "#     sub_level_inputs = tokenizer.encode_plus(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "#     sub_level_outputs = model(sub_level_inputs[\"input_ids\"], attention_mask=sub_level_inputs[\"attention_mask\"])\n",
    "#     sub_level_predicted_label = torch.argmax(sub_level_outputs.logits).item()\n",
    "#     # sub_level_predicted_intents = [k for k, v in sub_level_label_map.items() if v in sub_level_predicted_labels]\n",
    "\n",
    "#     print(sub_level_label_map)\n",
    "\n",
    "#     sub_level_predicted_intent = [k for k, v in sub_level_label_map.items() if v == sub_level_predicted_label]\n",
    "    \n",
    "    \n",
    "#     print(\"predicted sublevel intents\", sub_level_predicted_intent)\n",
    "\n",
    "#     return top_level_predicted_intent[0] if top_level_predicted_intent else None, sub_level_predicted_intent[0] if sub_level_predicted_intent else None\n",
    "    \n",
    "\n",
    "top_level_intents, sub_level_intents = predict_intent(\"What information does the privacy policy collect?\")\n",
    "print(top_level_intents, sub_level_intents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "import random\n",
    "\n",
    "# load the pre-trained intent analysis model\n",
    "# nlp = spacy.load(\"en_trf_bertbaseuncased_lg\")\n",
    "\n",
    "response_map = {\n",
    "    (\"security\", \"security_relating_to\"): [\"Our security measures include...\", \"We take security very seriously and have implemented...\"],\n",
    "    (\"security\", \"security_concerns\"): [\"We understand your security concerns and have taken steps to address them.\", \"You can trust that your information is safe with us.\"],\n",
    "    (\"information\", \"information_about\"): [\"Our store offers a variety of products, including...\", \"We also have a rewards program that allows you to earn points on your purchases.\"],\n",
    "    (\"information\", \"information_schedule\"): [\"We are open from 9am to 10pm, 7 days a week.\", \"Our business hours are 9am to 5pm, Monday to Friday.\"],\n",
    "    (\"help\", \"help_with_finding\"): [\"Here are some hotels near the airport:...\", \"I can help you find a hotel that meets your needs.\"],\n",
    "    (\"help\", \"help_with_booking\"): [\"You can book a room on our website or by calling our reservation hotline.\", \"We also offer a loyalty program that gives you discounts on future bookings.\"],\n",
    "    (\"information\", \"ordering\"): [\"You can place an order on our website or by calling our order hotline.\", \"We also offer a loyalty program that gives you discounts on future orders.\"]\n",
    "}\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def user(user_message, history):\n",
    "        return \"\", history + [[user_message, None]]\n",
    "\n",
    "    def bot(history):\n",
    "        user_message = history[-1][0]\n",
    "\n",
    "        intent = predict_intent(text=user_message)\n",
    "        print('intent:', intent)\n",
    "        # Random choice randomly chooses one of the options that matches the intent\n",
    "     \n",
    "        # generating a response with GPT if the main intent was 'privacy_policy' or 'legal_statement'\n",
    "        use_gpt = intent[0] == 'privacy_policy' or intent[0] == 'legal_statement'\n",
    "\n",
    "        response = gpt_model.answer_question(question=user_message) if use_gpt else 'No idea, bitch'\n",
    "\n",
    "        # response = random.choice(response_map[intent])\n",
    "        history[-1][1] = response\n",
    "        # The sleep is to simulate a more natural conversation\n",
    "        time.sleep(1)\n",
    "        return history\n",
    "\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "        bot, chatbot, chatbot\n",
    "    )\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT model herel \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from gpt import GPT\n",
    "\n",
    "gpt_model = GPT()\n",
    "\n",
    "#gpt_model.answer_question(question='What is the most important thing I need to know about your privacy statement?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT query function here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "import random\n",
    "\n",
    "# load the pre-trained intent analysis model\n",
    "# nlp = spacy.load(\"en_trf_bertbaseuncased_lg\")\n",
    "\n",
    "response_map = {\n",
    "    (\"security\", \"security_relating_to\"): [\"Our security measures include...\", \"We take security very seriously and have implemented...\"],\n",
    "    (\"security\", \"security_concerns\"): [\"We understand your security concerns and have taken steps to address them.\", \"You can trust that your information is safe with us.\"],\n",
    "    (\"information\", \"information_about\"): [\"Our store offers a variety of products, including...\", \"We also have a rewards program that allows you to earn points on your purchases.\"],\n",
    "    (\"information\", \"information_schedule\"): [\"We are open from 9am to 10pm, 7 days a week.\", \"Our business hours are 9am to 5pm, Monday to Friday.\"],\n",
    "    (\"help\", \"help_with_finding\"): [\"Here are some hotels near the airport:...\", \"I can help you find a hotel that meets your needs.\"],\n",
    "    (\"help\", \"help_with_booking\"): [\"You can book a room on our website or by calling our reservation hotline.\", \"We also offer a loyalty program that gives you discounts on future bookings.\"],\n",
    "    (\"information\", \"ordering\"): [\"You can place an order on our website or by calling our order hotline.\", \"We also offer a loyalty program that gives you discounts on future orders.\"]\n",
    "}\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def user(user_message, history):\n",
    "        return \"\", history + [[user_message, None]]\n",
    "\n",
    "    def bot(history):\n",
    "        user_message = history[-1][0]\n",
    "\n",
    "        intent = predict_intent(text=user_message)\n",
    "        print('intent:', intent)\n",
    "        # Random choice randomly chooses one of the options that matches the intent\n",
    "     \n",
    "        # generating a response with GPT if the main intent was 'privacy_policy' or 'legal_statement'\n",
    "        use_gpt = intent[0] == 'privacy_policy' or intent[0] == 'legal_statement'\n",
    "\n",
    "        response = gpt_model.answer_question(question=user_message) if use_gpt else 'No idea, bitch'\n",
    "\n",
    "        # response = random.choice(response_map[intent])\n",
    "        history[-1][1] = response\n",
    "        # The sleep is to simulate a more natural conversation\n",
    "        time.sleep(1)\n",
    "        return history\n",
    "\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "        bot, chatbot, chatbot\n",
    "    )\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
