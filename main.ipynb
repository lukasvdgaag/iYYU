{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71db93361f7c46da8fa5d9b480aefb91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gaagj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gaagj\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22aeae028d3d4054acf3fd06f5b7248b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348edb694c2343019eb89c7ab1086748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c9a21731c34dfa9cc49583bfa2eafa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.461476445198059\n",
      "Epoch 2, Loss: 1.285217046737671\n",
      "Epoch 3, Loss: 1.1575936079025269\n",
      "Epoch 4, Loss: 1.0369764566421509\n",
      "Epoch 5, Loss: 0.9474220275878906\n",
      "Epoch 6, Loss: 0.8697894811630249\n",
      "Epoch 7, Loss: 0.8198726773262024\n",
      "Epoch 8, Loss: 0.7558162212371826\n",
      "Epoch 9, Loss: 0.7613324522972107\n",
      "Epoch 10, Loss: 0.7010558843612671\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "import json\n",
    "\n",
    "# Load the data from JSON file\n",
    "with open(\"sentiment_analysis.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the training data\n",
    "train_data = []\n",
    "sub_labels = {}\n",
    "for top_level_label, sub_level_labels in data.items():\n",
    "    for sub_level_label_data in sub_level_labels:\n",
    "        text = sub_level_label_data[\"text\"]\n",
    "        sub_level_label = sub_level_label_data[\"sub_level_label\"]\n",
    "        train_data.append((text, top_level_label, sub_level_label))\n",
    "        if top_level_label not in sub_labels:\n",
    "            sub_labels[top_level_label] = []\n",
    "        sub_labels[top_level_label].append(sub_level_label)\n",
    "\n",
    "# Define the mapping between top-level labels and integers\n",
    "top_level_label_map = {label: i for i, label in enumerate(set([data[1] for data in train_data]))}\n",
    "\n",
    "# Define the mapping between sub-level labels and integers\n",
    "sub_level_label_map = {sub_label: i for i, sub_label in enumerate(set([sub_label for sub_labels_list in sub_labels.values() for sub_label in sub_labels_list]))}\n",
    "\n",
    "# Convert the training data labels to integers using the label_map and sub_label_map\n",
    "# A tensor is a multi-dimensional array that looks like a numpy array, it's used for neural networks\n",
    "top_level_labels = torch.tensor([top_level_label_map[data[1]] for data in train_data])\n",
    "sub_level_labels = torch.tensor([sub_level_label_map[sub_label] for data in train_data for sub_label in sub_labels[data[1]]])\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(top_level_label_map))\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the training data and convert to tensors\n",
    "inputs = tokenizer.batch_encode_plus([data[0] for data in train_data], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Fine-tune the model on the training data\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], labels=top_level_labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Evaluate the model on the training data\n",
    "    predictions = outputs.logits.argmax(axis=1)\n",
    "    accuracy = (predictions == top_level_labels).sum()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis query function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['platform_settings', 'privacy_policy', 'external_platform_settings', 'legal_statement'] ['question_answering', 'summarization']\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "text = \"Where can I change my account password?\"\n",
    "\n",
    "def predict_intent(text):\n",
    "    top_level_inputs = tokenizer.encode_plus(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    top_level_outputs = model(top_level_inputs[\"input_ids\"], attention_mask=top_level_inputs[\"attention_mask\"])\n",
    "    top_level_predicted_labels = torch.argsort(top_level_outputs.logits, descending=True).tolist()[0]\n",
    "    top_level_predicted_intents = [k for k, v in top_level_label_map.items() if v in top_level_predicted_labels]\n",
    "    \n",
    "    sub_level_inputs = tokenizer.encode_plus(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    sub_level_outputs = model(sub_level_inputs[\"input_ids\"], attention_mask=sub_level_inputs[\"attention_mask\"])\n",
    "    sub_level_predicted_labels = torch.argsort(sub_level_outputs.logits, descending=True).tolist()[0]\n",
    "    sub_level_predicted_intents = [k for k, v in sub_level_label_map.items() if v in sub_level_predicted_labels]\n",
    "    \n",
    "    return top_level_predicted_intents, sub_level_predicted_intents\n",
    "\n",
    "\n",
    "\n",
    "top_level_intents, sub_level_intents = predict_intent(text)\n",
    "print(top_level_intents, sub_level_intents)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\school\\iYYU\\gpt.py:23: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  serie = serie.str.replace('\\\\n', ' ')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'We are committed to securing your personal information, but we cannot guarantee that unauthorized third parties will never be able to access it. You have the right to ask for an overview of the information we have about you, correct or delete certain data, transfer some of this information to other organizations, withdraw your consent, and object to and restrict certain processing of your information.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPT model herel \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from gpt import GPT\n",
    "\n",
    "model = GPT()\n",
    "\n",
    "model.answer_question(question='What is the most important thing I need to know about your privacy statement?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT query function here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, I don't have a response for that.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'then'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 53\u001b[0m\n\u001b[0;32m     48\u001b[0m         time\u001b[39m.\u001b[39msleep(\u001b[39m1\u001b[39m)\n\u001b[0;32m     49\u001b[0m         \u001b[39mreturn\u001b[39;00m history\n\u001b[1;32m---> 53\u001b[0m     msg\u001b[39m.\u001b[39;49msubmit(user, [msg, chatbot], [msg, chatbot], queue\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\u001b[39m.\u001b[39;49mthen(\n\u001b[0;32m     54\u001b[0m         bot, chatbot, chatbot\n\u001b[0;32m     55\u001b[0m     )\n\u001b[0;32m     56\u001b[0m     clear\u001b[39m.\u001b[39mclick(\u001b[39mlambda\u001b[39;00m: \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, chatbot, queue\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     58\u001b[0m demo\u001b[39m.\u001b[39mlaunch()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'then'"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "import random\n",
    "\n",
    "# load the pre-trained intent analysis model\n",
    "# nlp = spacy.load(\"en_trf_bertbaseuncased_lg\")\n",
    "\n",
    "response_map = {\n",
    "    (\"security\", \"security_relating_to\"): [\"Our security measures include...\", \"We take security very seriously and have implemented...\"],\n",
    "    (\"security\", \"security_concerns\"): [\"We understand your security concerns and have taken steps to address them.\", \"You can trust that your information is safe with us.\"],\n",
    "    (\"ordering\", \"ordering_type_of_food\"): [\"Our menu features a variety of Italian dishes, including pizza and pasta.\", \"We also offer a selection of salads and appetizers.\"],\n",
    "    (\"ordering\", \"ordering_delivery\"): [\"You can place a delivery order on our website or by calling our delivery hotline.\", \"Delivery is available within a 10-mile radius of our store.\"],\n",
    "    (\"information\", \"information_about\"): [\"Our store offers a variety of products, including...\", \"We also have a rewards program that allows you to earn points on your purchases.\"],\n",
    "    (\"information\", \"information_schedule\"): [\"We are open from 9am to 10pm, 7 days a week.\", \"Our business hours are 9am to 5pm, Monday to Friday.\"],\n",
    "    (\"help\", \"help_with_finding\"): [\"Here are some hotels near the airport:...\", \"I can help you find a hotel that meets your needs.\"],\n",
    "    (\"help\", \"help_with_booking\"): [\"You can book a room on our website or by calling our reservation hotline.\", \"We also offer a loyalty program that gives you discounts on future bookings.\"],\n",
    "    (\"information\", \"ordering\"): [\"You can place an order on our website or by calling our order hotline.\", \"We also offer a loyalty program that gives you discounts on future orders.\"]\n",
    "}\n",
    "\n",
    "# Example usage\n",
    "intent = \"information\"\n",
    "sub_intent = \"ordering_delivery\"\n",
    "\n",
    "try:\n",
    "    response = random.choice(response_map[(intent, sub_intent)])\n",
    "except KeyError:\n",
    "    response = \"I'm sorry, I don't have a response for that.\"\n",
    "\n",
    "print(response)\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def user(user_message, history):\n",
    "        return \"\", history + [[user_message, None]]\n",
    "\n",
    "    def bot(history):\n",
    "        user_message = history[-1][0]\n",
    "        predicted_intent = predict_intent(user_message)\n",
    "        # Random choice randomly chooses one of the options that matches the intent\n",
    "     \n",
    "        response = random.choice(response_map[predicted_intent])\n",
    "        history[-1][1] = response\n",
    "        # The sleep is to simulate a more natural conversation\n",
    "        time.sleep(1)\n",
    "        return history\n",
    "\n",
    "\n",
    "\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "        bot, chatbot, chatbot\n",
    "    )\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
