{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Intent recognition imports\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# ChatGPT imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from gpt import GPT\n",
    "\n",
    "# Chat demo\n",
    "import gradio as gr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intent recognition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.8007309436798096\n",
      "Epoch 2, Loss: 3.7969255447387695\n",
      "Epoch 3, Loss: 3.652984857559204\n",
      "Epoch 4, Loss: 3.7080342769622803\n",
      "Epoch 5, Loss: 3.635124921798706\n",
      "Epoch 6, Loss: 3.378645420074463\n",
      "Epoch 7, Loss: 3.0341482162475586\n",
      "Epoch 8, Loss: 3.211941957473755\n",
      "Epoch 9, Loss: 2.8290586471557617\n",
      "Epoch 10, Loss: 2.6684978008270264\n",
      "Epoch 11, Loss: 2.477515697479248\n",
      "Epoch 12, Loss: 2.1890857219696045\n",
      "Epoch 13, Loss: 1.9279072284698486\n",
      "Epoch 14, Loss: 1.7251923084259033\n",
      "Epoch 15, Loss: 1.5288735628128052\n",
      "Epoch 16, Loss: 1.344827651977539\n",
      "Epoch 17, Loss: 1.1490167379379272\n",
      "Epoch 18, Loss: 0.9751096963882446\n",
      "Epoch 19, Loss: 0.8410384654998779\n",
      "Epoch 20, Loss: 0.7184804081916809\n"
     ]
    }
   ],
   "source": [
    "# Load the data from JSON file\n",
    "with open(\"intent_recognition.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the training data\n",
    "train_data = []\n",
    "for datum in data:\n",
    "    texts = datum[\"text\"]\n",
    "    label = datum[\"intent\"]\n",
    "    for text in texts:\n",
    "        train_data.append((text, label))\n",
    "\n",
    "# Define the mapping between top-level labels and integers\n",
    "label_map = {label: i for i, label in enumerate(set([data[1] for data in train_data]))}\n",
    "\n",
    "# Convert the training data labels to integers using the label_map\n",
    "labels = torch.tensor([label_map[data[1]] for data in train_data])\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_map))\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the training data and convert to tensors\n",
    "inputs = tokenizer.batch_encode_plus([data[0] for data in train_data], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Fine-tune the model on the training data\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4) #default 5e-5\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "for epoch in range(20):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], labels=labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Evaluate the model on the training data\n",
    "    predictions = outputs.logits.argmax(axis=1)\n",
    "    accuracy = (predictions == labels).sum()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model after training it\n",
    "# torch.save(model, \"intent_recognition.pt\")\n",
    "\n",
    "# Load the model from a file\n",
    "# model = torch.load(\"intent_recognition.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query intent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted intent: liability_legal_statement\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'liability_legal_statement'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_intent(question):\n",
    "\n",
    "    # Tokenize the test question and convert to tensors\n",
    "    inputs = tokenizer.encode_plus(question, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Get the model's prediction for the test question\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "    predictions = outputs.logits.argmax(axis=1)\n",
    "    predicted_label = list(label_map.keys())[list(label_map.values()).index(predictions[0].item())]\n",
    "    print(f\"Predicted intent: {predicted_label}\")\n",
    "    return predicted_label\n",
    "\n",
    "# Sample test question\n",
    "test_question = \"Can you summarize the legal statement?\"\n",
    "get_intent(test_question)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT model here\n",
    "# gpt_model = GPT()\n",
    "\n",
    "gpt_model = False\n",
    "\n",
    "# Test ChatGPT model\n",
    "# gpt_model.answer_question(question='What is the most important thing I need to know about your privacy statement?')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get response using intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object_by_intent(intent):\n",
    "    for object in data:\n",
    "        if object['intent'] == intent:\n",
    "            return object\n",
    "    return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'user_id': '0', 'profile_published': False, 'open_to_connecting': False, 'visibile_in_search': False, 'findable_to': 'logged_on_only', 'individual_user_viewability': [{'user_id': '1', 'profile_card_component_0_visible': True, 'profile_card_component_1_visible': True, 'profile_card_component_2_visible': True, 'profile_card_component_3_visible': True, 'profile_card_component_4_visible': True}, {'user_id': '2', 'profile_card_component_0_visible': True, 'profile_card_component_1_visible': False, 'profile_card_component_2_visible': False, 'profile_card_component_3_visible': False, 'profile_card_component_4_visible': False}]}, {'user_id': '1', 'profile_published': True, 'open_to_connecting': True, 'visibile_in_search': True, 'findable_to': 'everyone'}, {'user_id': '2', 'profile_published': False, 'open_to_connecting': False, 'visibile_in_search': False, 'findable_to': 'logged_on_only'}, {'user_id': '3', 'profile_published': False, 'open_to_connecting': False, 'visibile_in_search': False, 'findable_to': 'logged_on_only'}]\n"
     ]
    }
   ],
   "source": [
    "# Load user_profiles.json\n",
    "with open('user_profiles.json', 'r') as file:\n",
    "    user_profiles = json.load(file)\n",
    "\n",
    "# Print the loaded data\n",
    "print(user_profiles)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading user security levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'profile_published': True, 'open_to_connecting': True, 'visibile_in_search': True, 'findable_to': 'everyone'}\n",
      "{'profile_published': False, 'open_to_connecting': True, 'visibile_in_search': True, 'findable_to': 'logged_on_only'}\n",
      "{'profile_published': False, 'open_to_connecting': False, 'visibile_in_search': False, 'findable_to': 'logged_on_only'}\n",
      "True\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "with open('user_security_levels.json', 'r') as file:\n",
    "    user_security_levels = json.load(file)\n",
    "\n",
    "# Accessing the values\n",
    "low_security = user_security_levels['low_security']\n",
    "medium_security = user_security_levels['medium_security']\n",
    "high_security = user_security_levels['high_security']\n",
    "\n",
    "# Example usage\n",
    "print(low_security)\n",
    "print(medium_security)\n",
    "print(high_security)\n",
    "\n",
    "print(print(low_security['profile_published'])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to edit user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_user_security_level(user_id):\n",
    "    '''\n",
    "    Set up this function to ask the user a bunch of questions, and then based on the answers choose a secutiry level that suits them\n",
    "    then use the above pre-defined user profile security levels to set their security settings\n",
    "    since this will be a rule based approach it should just be a bunch of if statements and use the gradio chat to communicate with the user\n",
    "    '''\n",
    "    return None\n",
    "\n",
    "def update_user_setting(user_id, update_data):\n",
    "    '''\n",
    "    This function should be called with the user_id and the setting that should be updated, the answer can be processed here to determine the new value it should have\n",
    "    or it can be done else where as long as the actual update is done here. This can be done by using a json functionality to update the file\n",
    "    '''\n",
    "    return None\n",
    "\n",
    "def set_user_calling_card_visibility(user_id, second_user_id):\n",
    "    '''\n",
    "    This function should be called with the user_id and the second_user_id and the calling card visibility should be updated\n",
    "    this can be done by using a json functionality to update the file. As can been seen in the user profiles json, each user has an array that contains objects\n",
    "    each of the objects contains the id of the user who's the settings are for, as well as the personal visbility for the target user\n",
    "\n",
    "    Example:\n",
    "    User Frank (id: 0) want User (Lukas id: 1) to only be able to see select elements of their calling card. This function does that by taking the user id's and updating the\n",
    "    calling card visiblities accordingly to the intent that's recognised\n",
    "    '''\n",
    "    return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up gradio chat for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted intent: opt_out_targeted_advertising\n",
      "intent: opt_out_targeted_advertising\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Program Files\\Python38\\lib\\site-packages\\gradio\\routes.py\", line 395, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Program Files\\Python38\\lib\\site-packages\\gradio\\blocks.py\", line 1193, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\Program Files\\Python38\\lib\\site-packages\\gradio\\blocks.py\", line 916, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"c:\\Program Files\\Python38\\lib\\site-packages\\anyio\\to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"c:\\Program Files\\Python38\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Program Files\\Python38\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\Frank\\AppData\\Local\\Temp\\ipykernel_16896\\2964325488.py\", line 77, in bot\n",
      "    response = '(ChatGPT - intent found: {}): \\n{}'.format(intent['intent'], gpt_model.answer_question(question=user_message)) if use_gpt else '(intent found: {}): \\n{}'.format(intent['intent'], intent['responses'][0])\n",
      "AttributeError: 'bool' object has no attribute 'answer_question'\n"
     ]
    }
   ],
   "source": [
    "theme = gr.themes.Soft(\n",
    "    primary_hue=\"orange\",\n",
    "    secondary_hue=\"orange\",\n",
    ").set(\n",
    "    body_text_color_dark='*neutral_800',\n",
    "    background_fill_primary_dark='*neutral_50',\n",
    "    background_fill_secondary_dark='*neutral_50',\n",
    "    border_color_accent_dark='*primary_300',\n",
    "    border_color_primary_dark='*neutral_200',\n",
    "    color_accent_soft_dark='*primary_50',\n",
    "    link_text_color_dark='*secondary_600',\n",
    "    link_text_color_active_dark='*secondary_600',\n",
    "    link_text_color_hover_dark='*secondary_700',\n",
    "    link_text_color_visited_dark='*secondary_500',\n",
    "    block_background_fill='*neutral_100',\n",
    "    block_background_fill_dark='*neutral_100',\n",
    "    block_label_background_fill='*primary_400',\n",
    "    block_label_background_fill_dark='*primary_400',\n",
    "    block_label_text_color='*neutral_50',\n",
    "    block_label_text_color_dark='*neutral_50',\n",
    "    block_title_text_color='*neutral_50',\n",
    "    block_title_text_color_dark='*neutral_50',\n",
    "    checkbox_background_color_dark='*background_fill_primary',\n",
    "    checkbox_background_color_selected='*primary_500',\n",
    "    checkbox_background_color_selected_dark='*primary_500',\n",
    "    checkbox_border_color_dark='*neutral_100',\n",
    "    checkbox_border_color_focus='*primary_300',\n",
    "    checkbox_border_color_focus_dark='*primary_300',\n",
    "    checkbox_border_color_hover_dark='*neutral_300',\n",
    "    checkbox_border_color_selected='*primary_500',\n",
    "    checkbox_border_color_selected_dark='*primary_500',\n",
    "    checkbox_border_width_dark='1px',\n",
    "    checkbox_label_background_fill_selected_dark='*primary_500',\n",
    "    checkbox_label_text_color_selected_dark='white',\n",
    "    error_background_fill_dark='#fee2e2',\n",
    "    error_border_color_dark='#fecaca',\n",
    "    input_background_fill_dark='white',\n",
    "    input_background_fill_focus_dark='*secondary_500',\n",
    "    input_border_color_dark='*neutral_50',\n",
    "    input_border_color_focus_dark='*secondary_300',\n",
    "    input_placeholder_color_dark='*neutral_400',\n",
    "    slider_color_dark='*primary_500',\n",
    "    stat_background_fill_dark='*primary_300',\n",
    "    table_border_color_dark='*neutral_300',\n",
    "    table_even_background_fill_dark='white',\n",
    "    table_odd_background_fill_dark='*neutral_50',\n",
    "    button_primary_background_fill_dark='*primary_500',\n",
    "    button_primary_background_fill_hover_dark='*primary_400',\n",
    "    button_primary_border_color_dark='*primary_200',\n",
    "    button_secondary_background_fill_dark='white',\n",
    "    button_secondary_background_fill_hover_dark='*neutral_100',\n",
    "    button_secondary_border_color_dark='*neutral_200',\n",
    "    button_secondary_text_color_dark='*neutral_800'\n",
    ")\n",
    "\n",
    "with gr.Blocks(theme=theme, css=\"chat/chat.css\") as demo:\n",
    "    gr.Image(\"https://iyyu.com/_nuxt/img/navbar_logoW@2x.79eba99.png\", interactive=False, tool=\"image\", show_label=False, elem_classes=\"logo\").style(width=200)\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    submit = gr.Button(\"Submit\")\n",
    "\n",
    "    def user(user_message, history):\n",
    "        return \"\", history + [[user_message, None]]\n",
    "\n",
    "    def bot(history):\n",
    "        user_message = history[-1][0]\n",
    "\n",
    "        intent = get_intent(question=user_message)\n",
    "        print('intent:', intent)\n",
    "        \n",
    "        intent =  get_object_by_intent(intent)\n",
    "        \n",
    "        # generating a response with GPT if the main intent was 'privacy_policy' or 'legal_statement'\n",
    "        use_gpt = intent['use_gpt']\n",
    "\n",
    "        # if the intent is 'privacy_policy' or 'legal_statement', use GPT to generate a response\n",
    "        response = '(ChatGPT - intent found: {}): \\n{}'.format(intent['intent'], gpt_model.answer_question(question=user_message)) if use_gpt else '(intent found: {}): \\n{}'.format(intent['intent'], intent['responses'][0])\n",
    "\n",
    "        # response = random.choice(response_map[intent])\n",
    "        history[-1][1] = response\n",
    "        # The sleep is to simulate a more natural conversation\n",
    "        time.sleep(1)\n",
    "        return history\n",
    "\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "        bot, chatbot, chatbot\n",
    "    )\n",
    "    submit.click(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "        bot, chatbot, chatbot\n",
    "    )\n",
    "    \n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
