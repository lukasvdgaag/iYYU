{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Define the training data\n",
    "train_data = [\n",
    "    (\"Hi, I would like to know about the security settings\", \"security\"),\n",
    "    (\"I want to order some italian food\", \"ordering\"),\n",
    "    (\"What time does the store open tomorrow?\", \"information\"),\n",
    "    (\"Can you help me find a hotel near the airport?\", \"help\"),\n",
    "]\n",
    "\n",
    "# Define sub-level labels for each top-level label\n",
    "sub_labels = {\n",
    "    \"security\": [\"security_relating_to\", \"security_concerns\"],\n",
    "    \"ordering\": [\"ordering_type_of_food\", \"ordering_delivery\"],\n",
    "    \"information\": [\"information_about\", \"information_schedule\"],\n",
    "    \"help\": [\"help_with_finding\", \"help_with_booking\"]\n",
    "}\n",
    "\n",
    "# Define the mapping between top-level labels and integers\n",
    "top_level_label_map = {label: i for i, label in enumerate(set([data[1] for data in train_data]))}\n",
    "\n",
    "# Define the mapping between sub-level labels and integers\n",
    "sub_level_label_map = {label: i for i, label in enumerate(set([sub_label for sub_labels_list in sub_labels.values() for sub_label in sub_labels_list]))}\n",
    "\n",
    "# Convert the training data labels to integers using the label_map and sub_label_map\n",
    "# A tensor is a multi-dimensional array that looks like a numpy array, it's used for neural networks\n",
    "top_level_labels = torch.tensor([top_level_label_map[data[1]] for data in train_data])\n",
    "sub_level_labels = torch.tensor([sub_level_label_map[sub_label] for label, sub_labels_list in sub_labels.items() for sub_label in sub_labels_list for data in train_data if data[1] == label])\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "top_level_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(top_level_label_map))\n",
    "sub_level_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(sub_level_label_map))\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the training data and convert to tensors\n",
    "top_level_inputs = tokenizer.batch_encode_plus([data[0] for data in train_data], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "sub_level_inputs = tokenizer.batch_encode_plus([data[0] for data in train_data for sub_label in sub_labels[data[1]]], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Fine-tune the models on the training data\n",
    "top_level_optimizer = torch.optim.Adam(top_level_model.parameters(), lr=1e-5)\n",
    "sub_level_optimizer = torch.optim.Adam(sub_level_model.parameters(), lr=1e-5)\n",
    "for epoch in range(10):\n",
    "    top_level_outputs = top_level_model(top_level_inputs[\"input_ids\"], attention_mask=top_level_inputs[\"attention_mask\"], labels=top_level_labels)\n",
    "    top_level_loss = top_level_outputs.loss\n",
    "    top_level_loss.backward()\n",
    "    top_level_optimizer.step()\n",
    "    top_level_optimizer.zero_grad()\n",
    "\n",
    "    sub_level_outputs = sub_level_model(sub_level_inputs[\"input_ids\"], attention_mask=sub_level_inputs[\"attention_mask\"], labels=sub_level_labels)\n",
    "    sub_level_loss = sub_level_outputs.loss\n",
    "    sub_level_loss.backward()\n",
    "    sub_level_optimizer.step()\n",
    "    sub_level_optimizer.zero_grad()\n",
    "\n",
    "    # print(f\"Epoch {epoch+1}, Top-Level Loss: {top_level_loss.item()}, Sub-Level Loss: {sub_level_loss.item()}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis query function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "information information_about\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "text = \"What time does the Italian restaurant open tomorrow?\"\n",
    "\n",
    "def predict_intent(text):\n",
    "    top_level_inputs = tokenizer.encode_plus(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    top_level_outputs = top_level_model(top_level_inputs[\"input_ids\"], attention_mask=top_level_inputs[\"attention_mask\"])\n",
    "    top_level_predicted_label = torch.argmax(top_level_outputs.logits).item()\n",
    "    top_level_predicted_intent = [k for k, v in top_level_label_map.items() if v == top_level_predicted_label][0]\n",
    "    \n",
    "    sub_level_inputs = tokenizer.encode_plus(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    sub_level_outputs = sub_level_model(sub_level_inputs[\"input_ids\"], attention_mask=sub_level_inputs[\"attention_mask\"])\n",
    "    sub_level_predicted_label = torch.argmax(sub_level_outputs.logits).item()\n",
    "    sub_level_predicted_intent = [k for k, v in sub_level_label_map.items() if v == sub_level_predicted_label][0]\n",
    "    \n",
    "    return top_level_predicted_intent, sub_level_predicted_intent\n",
    "\n",
    "top_level_intent, sub_level_intent = predict_intent(text)\n",
    "print(top_level_intent, sub_level_intent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT model herel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT query function here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, I don't have a response for that.\n",
      "Running on local URL:  http://127.0.0.1:7872\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7872/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Program Files\\Python38\\lib\\site-packages\\gradio\\routes.py\", line 395, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Program Files\\Python38\\lib\\site-packages\\gradio\\blocks.py\", line 1193, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\Program Files\\Python38\\lib\\site-packages\\gradio\\blocks.py\", line 916, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"c:\\Program Files\\Python38\\lib\\site-packages\\anyio\\to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"c:\\Program Files\\Python38\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Program Files\\Python38\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\Frank\\AppData\\Local\\Temp\\ipykernel_20876\\3935284218.py\", line 45, in bot\n",
      "    response = random.choice(response_map[predicted_intent])\n",
      "KeyError: ('security', 'information_about')\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "import random\n",
    "\n",
    "# load the pre-trained intent analysis model\n",
    "# nlp = spacy.load(\"en_trf_bertbaseuncased_lg\")\n",
    "\n",
    "response_map = {\n",
    "    (\"security\", \"security_relating_to\"): [\"Our security measures include...\", \"We take security very seriously and have implemented...\"],\n",
    "    (\"security\", \"security_concerns\"): [\"We understand your security concerns and have taken steps to address them.\", \"You can trust that your information is safe with us.\"],\n",
    "    (\"ordering\", \"ordering_type_of_food\"): [\"Our menu features a variety of Italian dishes, including pizza and pasta.\", \"We also offer a selection of salads and appetizers.\"],\n",
    "    (\"ordering\", \"ordering_delivery\"): [\"You can place a delivery order on our website or by calling our delivery hotline.\", \"Delivery is available within a 10-mile radius of our store.\"],\n",
    "    (\"information\", \"information_about\"): [\"Our store offers a variety of products, including...\", \"We also have a rewards program that allows you to earn points on your purchases.\"],\n",
    "    (\"information\", \"information_schedule\"): [\"We are open from 9am to 10pm, 7 days a week.\", \"Our business hours are 9am to 5pm, Monday to Friday.\"],\n",
    "    (\"help\", \"help_with_finding\"): [\"Here are some hotels near the airport:...\", \"I can help you find a hotel that meets your needs.\"],\n",
    "    (\"help\", \"help_with_booking\"): [\"You can book a room on our website or by calling our reservation hotline.\", \"We also offer a loyalty program that gives you discounts on future bookings.\"],\n",
    "    (\"information\", \"ordering\"): [\"You can place an order on our website or by calling our order hotline.\", \"We also offer a loyalty program that gives you discounts on future orders.\"]\n",
    "}\n",
    "\n",
    "# Example usage\n",
    "intent = \"information\"\n",
    "sub_intent = \"ordering_delivery\"\n",
    "\n",
    "try:\n",
    "    response = random.choice(response_map[(intent, sub_intent)])\n",
    "except KeyError:\n",
    "    response = \"I'm sorry, I don't have a response for that.\"\n",
    "\n",
    "print(response)\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def user(user_message, history):\n",
    "        return \"\", history + [[user_message, None]]\n",
    "\n",
    "    def bot(history):\n",
    "        user_message = history[-1][0]\n",
    "        predicted_intent = predict_intent(user_message)\n",
    "        # Random choice randomly chooses one of the options that matches the intent\n",
    "     \n",
    "        response = random.choice(response_map[predicted_intent])\n",
    "        history[-1][1] = response\n",
    "        # The sleep is to simulate a more natural conversation\n",
    "        time.sleep(1)\n",
    "        return history\n",
    "\n",
    "\n",
    "\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "        bot, chatbot, chatbot\n",
    "    )\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a\n",
    "\n",
    "#### Source:\n",
    "\n",
    " https://www.section.io/engineering-education/intent-classification-with-rasa-and-spacy/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
