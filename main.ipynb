{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.414095163345337\n",
      "Epoch 2, Loss: 1.2877216339111328\n",
      "Epoch 3, Loss: 1.1811878681182861\n",
      "Epoch 4, Loss: 1.062727451324463\n",
      "Epoch 5, Loss: 0.9914543032646179\n",
      "Epoch 6, Loss: 0.9207535982131958\n",
      "Epoch 7, Loss: 0.8304901123046875\n",
      "Epoch 8, Loss: 0.7762413024902344\n",
      "Epoch 9, Loss: 0.7302121520042419\n",
      "Epoch 10, Loss: 0.6672614216804504\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# import json\n",
    "\n",
    "# # Load the data from JSON file\n",
    "# with open(\"sentiment_analysis.json\", \"r\") as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# print(data.items)\n",
    "\n",
    "# # Define the training data\n",
    "# train_data = []\n",
    "# sub_labels = {}\n",
    "# for top_level_label, sub_level_labels in data.items():\n",
    "#     for sub_level_label_data in sub_level_labels:\n",
    "#         text = sub_level_label_data[\"text\"]\n",
    "#         sub_level_label = sub_level_label_data[\"sub_level_label\"]\n",
    "#         train_data.append((text, top_level_label, sub_level_label))\n",
    "#         if top_level_label not in sub_labels:\n",
    "#             sub_labels[top_level_label] = []\n",
    "#         sub_labels[top_level_label].append(sub_level_label)\n",
    "\n",
    "\n",
    "\n",
    "# # Define the mapping between top-level labels and integers\n",
    "# top_level_label_map = {label: i for i, label in enumerate(set([data[1] for data in train_data]))}\n",
    "\n",
    "# # Define the mapping between sub-level labels and integers\n",
    "# sub_level_label_map = {sub_label: i for i, sub_label in enumerate(set([sub_label for sub_labels_list in sub_labels.values() for sub_label in sub_labels_list]))}\n",
    "\n",
    "# # Convert the training data labels to integers using the label_map and sub_label_map\n",
    "# # A tensor is a multi-dimensional array that looks like a numpy array, it's used for neural networks\n",
    "# top_level_labels = torch.tensor([top_level_label_map[data[1]] for data in train_data])\n",
    "# sub_level_labels = torch.tensor([sub_level_label_map[sub_label] for data in train_data for sub_label in sub_labels[data[1]]])\n",
    "\n",
    "# # Load the pre-trained BERT model and tokenizer\n",
    "# model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(top_level_label_map))\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# # Tokenize the training data and convert to tensors\n",
    "# inputs = tokenizer.batch_encode_plus([data[0] for data in train_data], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# # Fine-tune the model on the training data\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# for epoch in range(10):\n",
    "#     optimizer.zero_grad()\n",
    "#     outputs = model(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], labels=top_level_labels)\n",
    "#     loss = outputs.loss\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "\n",
    "#     print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "#     # Evaluate the model on the training data\n",
    "#     predictions = outputs.logits.argmax(axis=1)\n",
    "#     accuracy = (predictions == top_level_labels).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.757695198059082\n",
      "Epoch 2, Loss: 3.6782033443450928\n",
      "Epoch 3, Loss: 3.6343538761138916\n",
      "Epoch 4, Loss: 3.5965120792388916\n",
      "Epoch 5, Loss: 3.5506274700164795\n",
      "Epoch 6, Loss: 3.4822354316711426\n",
      "Epoch 7, Loss: 3.4257144927978516\n",
      "Epoch 8, Loss: 3.3681063652038574\n",
      "Epoch 9, Loss: 3.320793628692627\n",
      "Epoch 10, Loss: 3.2493276596069336\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import json\n",
    "\n",
    "# Load the data from JSON file\n",
    "with open(\"sentiment_analysis_v2.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the training data\n",
    "train_data = []\n",
    "for datum in data:\n",
    "    texts = datum[\"text\"]\n",
    "    label = datum[\"intent\"]\n",
    "    for text in texts:\n",
    "        train_data.append((text, label))\n",
    "\n",
    "# Define the mapping between top-level labels and integers\n",
    "label_map = {label: i for i, label in enumerate(set([data[1] for data in train_data]))}\n",
    "\n",
    "# Convert the training data labels to integers using the label_map\n",
    "labels = torch.tensor([label_map[data[1]] for data in train_data])\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_map))\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the training data and convert to tensors\n",
    "inputs = tokenizer.batch_encode_plus([data[0] for data in train_data], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Fine-tune the model on the training data\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], labels=labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Evaluate the model on the training data\n",
    "    predictions = outputs.logits.argmax(axis=1)\n",
    "    accuracy = (predictions == labels).sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted intent: update_password_change\n"
     ]
    }
   ],
   "source": [
    "# Sample test question\n",
    "test_question = \"Why can't I change my password?\"\n",
    "\n",
    "# Tokenize the test question and convert to tensors\n",
    "inputs = tokenizer.encode_plus(test_question, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Get the model's prediction for the test question\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "predictions = outputs.logits.argmax(axis=1)\n",
    "predicted_label = list(label_map.keys())[list(label_map.values()).index(predictions[0].item())]\n",
    "\n",
    "print(f\"Predicted intent: {predicted_label}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis query function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None contact\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "\n",
    "def predict_intent(text):\n",
    "    top_level_inputs = tokenizer.encode_plus(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    top_level_outputs = model(top_level_inputs[\"input_ids\"], attention_mask=top_level_inputs[\"attention_mask\"])\n",
    "    # top_level_predicted_labels = torch.argsort(top_level_outputs.logits, descending=True).tolist()[0]\n",
    "    top_level_predicted_label = torch.argmax(top_level_outputs.logits).item()\n",
    "    top_level_predicted_intent = [k for k, v in top_level_label_map.items() if v == top_level_predicted_label]\n",
    "    \n",
    "    sub_level_inputs = tokenizer.encode_plus(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    sub_level_outputs = model(sub_level_inputs[\"input_ids\"], attention_mask=sub_level_inputs[\"attention_mask\"])\n",
    "    sub_level_predicted_label = torch.argmax(sub_level_outputs.logits).item()\n",
    "    # sub_level_predicted_intents = [k for k, v in sub_level_label_map.items() if v in sub_level_predicted_labels]\n",
    "    sub_level_predicted_intent = [k for k, v in sub_level_label_map.items() if v == sub_level_predicted_label]\n",
    "    \n",
    "    return top_level_predicted_intent[0] if top_level_predicted_intent else None, sub_level_predicted_intent[0] if sub_level_predicted_intent else None\n",
    "\n",
    "\n",
    "\n",
    "top_level_intents, sub_level_intents = predict_intent(\"How can i change my password\")\n",
    "print(top_level_intents, sub_level_intents)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[51], line 7\u001B[0m\n\u001B[1;32m      3\u001B[0m get_ipython()\u001B[39m.\u001B[39mrun_line_magic(\u001B[39m'\u001B[39m\u001B[39mautoreload\u001B[39m\u001B[39m'\u001B[39m, \u001B[39m'\u001B[39m\u001B[39m2\u001B[39m\u001B[39m'\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mgpt\u001B[39;00m \u001B[39mimport\u001B[39;00m GPT\n\u001B[0;32m----> 7\u001B[0m gpt_model \u001B[39m=\u001B[39m GPT()\n\u001B[1;32m      9\u001B[0m \u001B[39m#gpt_model.answer_question(question='What is the most important thing I need to know about your privacy statement?')\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Documenten/HVA/Project AAI/iYYU/gpt.py:19\u001B[0m, in \u001B[0;36mGPT.__init__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     16\u001B[0m openai\u001B[39m.\u001B[39mapi_key \u001B[39m=\u001B[39m os\u001B[39m.\u001B[39mgetenv(\u001B[39m'\u001B[39m\u001B[39mOPENAI_API_KEY\u001B[39m\u001B[39m'\u001B[39m)\n\u001B[1;32m     18\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mload_files()\n\u001B[0;32m---> 19\u001B[0m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mdata_preprocessing()\n",
      "File \u001B[0;32m~/Documents/Documenten/HVA/Project AAI/iYYU/gpt.py:76\u001B[0m, in \u001B[0;36mGPT.data_preprocessing\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     73\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdf \u001B[39m=\u001B[39m pd\u001B[39m.\u001B[39mDataFrame(shortened, columns\u001B[39m=\u001B[39m[\u001B[39m'\u001B[39m\u001B[39mtext\u001B[39m\u001B[39m'\u001B[39m])\n\u001B[1;32m     74\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdf[\u001B[39m'\u001B[39m\u001B[39mn_tokens\u001B[39m\u001B[39m'\u001B[39m] \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdf\u001B[39m.\u001B[39mtext\u001B[39m.\u001B[39mapply(\u001B[39mlambda\u001B[39;00m x: \u001B[39mlen\u001B[39m(tokenizer\u001B[39m.\u001B[39mencode(x)))\n\u001B[0;32m---> 76\u001B[0m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mcreate_embeddings()\n",
      "File \u001B[0;32m~/Documents/Documenten/HVA/Project AAI/iYYU/gpt.py:114\u001B[0m, in \u001B[0;36mGPT.create_embeddings\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mcreate_embeddings\u001B[39m(\u001B[39mself\u001B[39m):\n\u001B[0;32m--> 114\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdf[\u001B[39m'\u001B[39m\u001B[39membeddings\u001B[39m\u001B[39m'\u001B[39m] \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mdf\u001B[39m.\u001B[39;49mtext\u001B[39m.\u001B[39;49mapply(\n\u001B[1;32m    115\u001B[0m         \u001B[39mlambda\u001B[39;49;00m x: openai\u001B[39m.\u001B[39;49mEmbedding\u001B[39m.\u001B[39;49mcreate(\u001B[39minput\u001B[39;49m\u001B[39m=\u001B[39;49mx, engine\u001B[39m=\u001B[39;49m\u001B[39m'\u001B[39;49m\u001B[39mtext-embedding-ada-002\u001B[39;49m\u001B[39m'\u001B[39;49m)[\u001B[39m'\u001B[39;49m\u001B[39mdata\u001B[39;49m\u001B[39m'\u001B[39;49m][\u001B[39m0\u001B[39;49m][\u001B[39m'\u001B[39;49m\u001B[39membedding\u001B[39;49m\u001B[39m'\u001B[39;49m])\n\u001B[1;32m    116\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdf\u001B[39m.\u001B[39mto_csv(\u001B[39m'\u001B[39m\u001B[39mprocessed/embeddings.csv\u001B[39m\u001B[39m'\u001B[39m)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/series.py:4631\u001B[0m, in \u001B[0;36mSeries.apply\u001B[0;34m(self, func, convert_dtype, args, **kwargs)\u001B[0m\n\u001B[1;32m   4521\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mapply\u001B[39m(\n\u001B[1;32m   4522\u001B[0m     \u001B[39mself\u001B[39m,\n\u001B[1;32m   4523\u001B[0m     func: AggFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4526\u001B[0m     \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs,\n\u001B[1;32m   4527\u001B[0m ) \u001B[39m-\u001B[39m\u001B[39m>\u001B[39m DataFrame \u001B[39m|\u001B[39m Series:\n\u001B[1;32m   4528\u001B[0m \u001B[39m    \u001B[39m\u001B[39m\"\"\"\u001B[39;00m\n\u001B[1;32m   4529\u001B[0m \u001B[39m    Invoke function on values of Series.\u001B[39;00m\n\u001B[1;32m   4530\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4629\u001B[0m \u001B[39m    dtype: float64\u001B[39;00m\n\u001B[1;32m   4630\u001B[0m \u001B[39m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 4631\u001B[0m     \u001B[39mreturn\u001B[39;00m SeriesApply(\u001B[39mself\u001B[39;49m, func, convert_dtype, args, kwargs)\u001B[39m.\u001B[39;49mapply()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/apply.py:1025\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1022\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mapply_str()\n\u001B[1;32m   1024\u001B[0m \u001B[39m# self.f is Callable\u001B[39;00m\n\u001B[0;32m-> 1025\u001B[0m \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mapply_standard()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/apply.py:1076\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1074\u001B[0m     \u001B[39melse\u001B[39;00m:\n\u001B[1;32m   1075\u001B[0m         values \u001B[39m=\u001B[39m obj\u001B[39m.\u001B[39mastype(\u001B[39mobject\u001B[39m)\u001B[39m.\u001B[39m_values\n\u001B[0;32m-> 1076\u001B[0m         mapped \u001B[39m=\u001B[39m lib\u001B[39m.\u001B[39;49mmap_infer(\n\u001B[1;32m   1077\u001B[0m             values,\n\u001B[1;32m   1078\u001B[0m             f,\n\u001B[1;32m   1079\u001B[0m             convert\u001B[39m=\u001B[39;49m\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mconvert_dtype,\n\u001B[1;32m   1080\u001B[0m         )\n\u001B[1;32m   1082\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mlen\u001B[39m(mapped) \u001B[39mand\u001B[39;00m \u001B[39misinstance\u001B[39m(mapped[\u001B[39m0\u001B[39m], ABCSeries):\n\u001B[1;32m   1083\u001B[0m     \u001B[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[1;32m   1084\u001B[0m     \u001B[39m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[1;32m   1085\u001B[0m     \u001B[39mreturn\u001B[39;00m obj\u001B[39m.\u001B[39m_constructor_expanddim(\u001B[39mlist\u001B[39m(mapped), index\u001B[39m=\u001B[39mobj\u001B[39m.\u001B[39mindex)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2834\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/Documents/Documenten/HVA/Project AAI/iYYU/gpt.py:115\u001B[0m, in \u001B[0;36mGPT.create_embeddings.<locals>.<lambda>\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mcreate_embeddings\u001B[39m(\u001B[39mself\u001B[39m):\n\u001B[1;32m    114\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdf[\u001B[39m'\u001B[39m\u001B[39membeddings\u001B[39m\u001B[39m'\u001B[39m] \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdf\u001B[39m.\u001B[39mtext\u001B[39m.\u001B[39mapply(\n\u001B[0;32m--> 115\u001B[0m         \u001B[39mlambda\u001B[39;00m x: openai\u001B[39m.\u001B[39;49mEmbedding\u001B[39m.\u001B[39;49mcreate(\u001B[39minput\u001B[39;49m\u001B[39m=\u001B[39;49mx, engine\u001B[39m=\u001B[39;49m\u001B[39m'\u001B[39;49m\u001B[39mtext-embedding-ada-002\u001B[39;49m\u001B[39m'\u001B[39;49m)[\u001B[39m'\u001B[39m\u001B[39mdata\u001B[39m\u001B[39m'\u001B[39m][\u001B[39m0\u001B[39m][\u001B[39m'\u001B[39m\u001B[39membedding\u001B[39m\u001B[39m'\u001B[39m])\n\u001B[1;32m    116\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdf\u001B[39m.\u001B[39mto_csv(\u001B[39m'\u001B[39m\u001B[39mprocessed/embeddings.csv\u001B[39m\u001B[39m'\u001B[39m)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/openai/api_resources/embedding.py:33\u001B[0m, in \u001B[0;36mEmbedding.create\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[39mwhile\u001B[39;00m \u001B[39mTrue\u001B[39;00m:\n\u001B[1;32m     32\u001B[0m     \u001B[39mtry\u001B[39;00m:\n\u001B[0;32m---> 33\u001B[0m         response \u001B[39m=\u001B[39m \u001B[39msuper\u001B[39;49m()\u001B[39m.\u001B[39;49mcreate(\u001B[39m*\u001B[39;49margs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m     35\u001B[0m         \u001B[39m# If a user specifies base64, we'll just return the encoded string.\u001B[39;00m\n\u001B[1;32m     36\u001B[0m         \u001B[39m# This is only for the default case.\u001B[39;00m\n\u001B[1;32m     37\u001B[0m         \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m user_provided_encoding_format:\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001B[0m, in \u001B[0;36mEngineAPIResource.create\u001B[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[39m@classmethod\u001B[39m\n\u001B[1;32m    128\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mcreate\u001B[39m(\n\u001B[1;32m    129\u001B[0m     \u001B[39mcls\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    136\u001B[0m     \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mparams,\n\u001B[1;32m    137\u001B[0m ):\n\u001B[1;32m    138\u001B[0m     (\n\u001B[1;32m    139\u001B[0m         deployment_id,\n\u001B[1;32m    140\u001B[0m         engine,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    150\u001B[0m         api_key, api_base, api_type, api_version, organization, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mparams\n\u001B[1;32m    151\u001B[0m     )\n\u001B[0;32m--> 153\u001B[0m     response, _, api_key \u001B[39m=\u001B[39m requestor\u001B[39m.\u001B[39;49mrequest(\n\u001B[1;32m    154\u001B[0m         \u001B[39m\"\u001B[39;49m\u001B[39mpost\u001B[39;49m\u001B[39m\"\u001B[39;49m,\n\u001B[1;32m    155\u001B[0m         url,\n\u001B[1;32m    156\u001B[0m         params\u001B[39m=\u001B[39;49mparams,\n\u001B[1;32m    157\u001B[0m         headers\u001B[39m=\u001B[39;49mheaders,\n\u001B[1;32m    158\u001B[0m         stream\u001B[39m=\u001B[39;49mstream,\n\u001B[1;32m    159\u001B[0m         request_id\u001B[39m=\u001B[39;49mrequest_id,\n\u001B[1;32m    160\u001B[0m         request_timeout\u001B[39m=\u001B[39;49mrequest_timeout,\n\u001B[1;32m    161\u001B[0m     )\n\u001B[1;32m    163\u001B[0m     \u001B[39mif\u001B[39;00m stream:\n\u001B[1;32m    164\u001B[0m         \u001B[39m# must be an iterator\u001B[39;00m\n\u001B[1;32m    165\u001B[0m         \u001B[39massert\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39misinstance\u001B[39m(response, OpenAIResponse)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/openai/api_requestor.py:216\u001B[0m, in \u001B[0;36mAPIRequestor.request\u001B[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001B[0m\n\u001B[1;32m    205\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mrequest\u001B[39m(\n\u001B[1;32m    206\u001B[0m     \u001B[39mself\u001B[39m,\n\u001B[1;32m    207\u001B[0m     method,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    214\u001B[0m     request_timeout: Optional[Union[\u001B[39mfloat\u001B[39m, Tuple[\u001B[39mfloat\u001B[39m, \u001B[39mfloat\u001B[39m]]] \u001B[39m=\u001B[39m \u001B[39mNone\u001B[39;00m,\n\u001B[1;32m    215\u001B[0m ) \u001B[39m-\u001B[39m\u001B[39m>\u001B[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001B[39mbool\u001B[39m, \u001B[39mstr\u001B[39m]:\n\u001B[0;32m--> 216\u001B[0m     result \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mrequest_raw(\n\u001B[1;32m    217\u001B[0m         method\u001B[39m.\u001B[39;49mlower(),\n\u001B[1;32m    218\u001B[0m         url,\n\u001B[1;32m    219\u001B[0m         params\u001B[39m=\u001B[39;49mparams,\n\u001B[1;32m    220\u001B[0m         supplied_headers\u001B[39m=\u001B[39;49mheaders,\n\u001B[1;32m    221\u001B[0m         files\u001B[39m=\u001B[39;49mfiles,\n\u001B[1;32m    222\u001B[0m         stream\u001B[39m=\u001B[39;49mstream,\n\u001B[1;32m    223\u001B[0m         request_id\u001B[39m=\u001B[39;49mrequest_id,\n\u001B[1;32m    224\u001B[0m         request_timeout\u001B[39m=\u001B[39;49mrequest_timeout,\n\u001B[1;32m    225\u001B[0m     )\n\u001B[1;32m    226\u001B[0m     resp, got_stream \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_interpret_response(result, stream)\n\u001B[1;32m    227\u001B[0m     \u001B[39mreturn\u001B[39;00m resp, got_stream, \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mapi_key\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/openai/api_requestor.py:516\u001B[0m, in \u001B[0;36mAPIRequestor.request_raw\u001B[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001B[0m\n\u001B[1;32m    514\u001B[0m     _thread_context\u001B[39m.\u001B[39msession \u001B[39m=\u001B[39m _make_session()\n\u001B[1;32m    515\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[0;32m--> 516\u001B[0m     result \u001B[39m=\u001B[39m _thread_context\u001B[39m.\u001B[39;49msession\u001B[39m.\u001B[39;49mrequest(\n\u001B[1;32m    517\u001B[0m         method,\n\u001B[1;32m    518\u001B[0m         abs_url,\n\u001B[1;32m    519\u001B[0m         headers\u001B[39m=\u001B[39;49mheaders,\n\u001B[1;32m    520\u001B[0m         data\u001B[39m=\u001B[39;49mdata,\n\u001B[1;32m    521\u001B[0m         files\u001B[39m=\u001B[39;49mfiles,\n\u001B[1;32m    522\u001B[0m         stream\u001B[39m=\u001B[39;49mstream,\n\u001B[1;32m    523\u001B[0m         timeout\u001B[39m=\u001B[39;49mrequest_timeout \u001B[39mif\u001B[39;49;00m request_timeout \u001B[39melse\u001B[39;49;00m TIMEOUT_SECS,\n\u001B[1;32m    524\u001B[0m         proxies\u001B[39m=\u001B[39;49m_thread_context\u001B[39m.\u001B[39;49msession\u001B[39m.\u001B[39;49mproxies,\n\u001B[1;32m    525\u001B[0m     )\n\u001B[1;32m    526\u001B[0m \u001B[39mexcept\u001B[39;00m requests\u001B[39m.\u001B[39mexceptions\u001B[39m.\u001B[39mTimeout \u001B[39mas\u001B[39;00m e:\n\u001B[1;32m    527\u001B[0m     \u001B[39mraise\u001B[39;00m error\u001B[39m.\u001B[39mTimeout(\u001B[39m\"\u001B[39m\u001B[39mRequest timed out: \u001B[39m\u001B[39m{}\u001B[39;00m\u001B[39m\"\u001B[39m\u001B[39m.\u001B[39mformat(e)) \u001B[39mfrom\u001B[39;00m \u001B[39me\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/requests/sessions.py:587\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    582\u001B[0m send_kwargs \u001B[39m=\u001B[39m {\n\u001B[1;32m    583\u001B[0m     \u001B[39m\"\u001B[39m\u001B[39mtimeout\u001B[39m\u001B[39m\"\u001B[39m: timeout,\n\u001B[1;32m    584\u001B[0m     \u001B[39m\"\u001B[39m\u001B[39mallow_redirects\u001B[39m\u001B[39m\"\u001B[39m: allow_redirects,\n\u001B[1;32m    585\u001B[0m }\n\u001B[1;32m    586\u001B[0m send_kwargs\u001B[39m.\u001B[39mupdate(settings)\n\u001B[0;32m--> 587\u001B[0m resp \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49msend(prep, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49msend_kwargs)\n\u001B[1;32m    589\u001B[0m \u001B[39mreturn\u001B[39;00m resp\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/requests/sessions.py:701\u001B[0m, in \u001B[0;36mSession.send\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    698\u001B[0m start \u001B[39m=\u001B[39m preferred_clock()\n\u001B[1;32m    700\u001B[0m \u001B[39m# Send the request\u001B[39;00m\n\u001B[0;32m--> 701\u001B[0m r \u001B[39m=\u001B[39m adapter\u001B[39m.\u001B[39;49msend(request, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m    703\u001B[0m \u001B[39m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[1;32m    704\u001B[0m elapsed \u001B[39m=\u001B[39m preferred_clock() \u001B[39m-\u001B[39m start\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/requests/adapters.py:489\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    487\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[1;32m    488\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m chunked:\n\u001B[0;32m--> 489\u001B[0m         resp \u001B[39m=\u001B[39m conn\u001B[39m.\u001B[39;49murlopen(\n\u001B[1;32m    490\u001B[0m             method\u001B[39m=\u001B[39;49mrequest\u001B[39m.\u001B[39;49mmethod,\n\u001B[1;32m    491\u001B[0m             url\u001B[39m=\u001B[39;49murl,\n\u001B[1;32m    492\u001B[0m             body\u001B[39m=\u001B[39;49mrequest\u001B[39m.\u001B[39;49mbody,\n\u001B[1;32m    493\u001B[0m             headers\u001B[39m=\u001B[39;49mrequest\u001B[39m.\u001B[39;49mheaders,\n\u001B[1;32m    494\u001B[0m             redirect\u001B[39m=\u001B[39;49m\u001B[39mFalse\u001B[39;49;00m,\n\u001B[1;32m    495\u001B[0m             assert_same_host\u001B[39m=\u001B[39;49m\u001B[39mFalse\u001B[39;49;00m,\n\u001B[1;32m    496\u001B[0m             preload_content\u001B[39m=\u001B[39;49m\u001B[39mFalse\u001B[39;49;00m,\n\u001B[1;32m    497\u001B[0m             decode_content\u001B[39m=\u001B[39;49m\u001B[39mFalse\u001B[39;49;00m,\n\u001B[1;32m    498\u001B[0m             retries\u001B[39m=\u001B[39;49m\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mmax_retries,\n\u001B[1;32m    499\u001B[0m             timeout\u001B[39m=\u001B[39;49mtimeout,\n\u001B[1;32m    500\u001B[0m         )\n\u001B[1;32m    502\u001B[0m     \u001B[39m# Send the request.\u001B[39;00m\n\u001B[1;32m    503\u001B[0m     \u001B[39melse\u001B[39;00m:\n\u001B[1;32m    504\u001B[0m         \u001B[39mif\u001B[39;00m \u001B[39mhasattr\u001B[39m(conn, \u001B[39m\"\u001B[39m\u001B[39mproxy_pool\u001B[39m\u001B[39m\"\u001B[39m):\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001B[0m\n\u001B[1;32m    700\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_prepare_proxy(conn)\n\u001B[1;32m    702\u001B[0m \u001B[39m# Make the request on the httplib connection object.\u001B[39;00m\n\u001B[0;32m--> 703\u001B[0m httplib_response \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_make_request(\n\u001B[1;32m    704\u001B[0m     conn,\n\u001B[1;32m    705\u001B[0m     method,\n\u001B[1;32m    706\u001B[0m     url,\n\u001B[1;32m    707\u001B[0m     timeout\u001B[39m=\u001B[39;49mtimeout_obj,\n\u001B[1;32m    708\u001B[0m     body\u001B[39m=\u001B[39;49mbody,\n\u001B[1;32m    709\u001B[0m     headers\u001B[39m=\u001B[39;49mheaders,\n\u001B[1;32m    710\u001B[0m     chunked\u001B[39m=\u001B[39;49mchunked,\n\u001B[1;32m    711\u001B[0m )\n\u001B[1;32m    713\u001B[0m \u001B[39m# If we're going to release the connection in ``finally:``, then\u001B[39;00m\n\u001B[1;32m    714\u001B[0m \u001B[39m# the response doesn't need to know about the connection. Otherwise\u001B[39;00m\n\u001B[1;32m    715\u001B[0m \u001B[39m# it will also try to release it and we'll have a double-release\u001B[39;00m\n\u001B[1;32m    716\u001B[0m \u001B[39m# mess.\u001B[39;00m\n\u001B[1;32m    717\u001B[0m response_conn \u001B[39m=\u001B[39m conn \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m release_conn \u001B[39melse\u001B[39;00m \u001B[39mNone\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/urllib3/connectionpool.py:449\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001B[0m\n\u001B[1;32m    444\u001B[0m             httplib_response \u001B[39m=\u001B[39m conn\u001B[39m.\u001B[39mgetresponse()\n\u001B[1;32m    445\u001B[0m         \u001B[39mexcept\u001B[39;00m \u001B[39mBaseException\u001B[39;00m \u001B[39mas\u001B[39;00m e:\n\u001B[1;32m    446\u001B[0m             \u001B[39m# Remove the TypeError from the exception chain in\u001B[39;00m\n\u001B[1;32m    447\u001B[0m             \u001B[39m# Python 3 (including for exceptions like SystemExit).\u001B[39;00m\n\u001B[1;32m    448\u001B[0m             \u001B[39m# Otherwise it looks like a bug in the code.\u001B[39;00m\n\u001B[0;32m--> 449\u001B[0m             six\u001B[39m.\u001B[39;49mraise_from(e, \u001B[39mNone\u001B[39;49;00m)\n\u001B[1;32m    450\u001B[0m \u001B[39mexcept\u001B[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001B[39mas\u001B[39;00m e:\n\u001B[1;32m    451\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_raise_timeout(err\u001B[39m=\u001B[39me, url\u001B[39m=\u001B[39murl, timeout_value\u001B[39m=\u001B[39mread_timeout)\n",
      "File \u001B[0;32m<string>:3\u001B[0m, in \u001B[0;36mraise_from\u001B[0;34m(value, from_value)\u001B[0m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/urllib3/connectionpool.py:444\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001B[0m\n\u001B[1;32m    441\u001B[0m \u001B[39mexcept\u001B[39;00m \u001B[39mTypeError\u001B[39;00m:\n\u001B[1;32m    442\u001B[0m     \u001B[39m# Python 3\u001B[39;00m\n\u001B[1;32m    443\u001B[0m     \u001B[39mtry\u001B[39;00m:\n\u001B[0;32m--> 444\u001B[0m         httplib_response \u001B[39m=\u001B[39m conn\u001B[39m.\u001B[39;49mgetresponse()\n\u001B[1;32m    445\u001B[0m     \u001B[39mexcept\u001B[39;00m \u001B[39mBaseException\u001B[39;00m \u001B[39mas\u001B[39;00m e:\n\u001B[1;32m    446\u001B[0m         \u001B[39m# Remove the TypeError from the exception chain in\u001B[39;00m\n\u001B[1;32m    447\u001B[0m         \u001B[39m# Python 3 (including for exceptions like SystemExit).\u001B[39;00m\n\u001B[1;32m    448\u001B[0m         \u001B[39m# Otherwise it looks like a bug in the code.\u001B[39;00m\n\u001B[1;32m    449\u001B[0m         six\u001B[39m.\u001B[39mraise_from(e, \u001B[39mNone\u001B[39;00m)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:1374\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1372\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[1;32m   1373\u001B[0m     \u001B[39mtry\u001B[39;00m:\n\u001B[0;32m-> 1374\u001B[0m         response\u001B[39m.\u001B[39;49mbegin()\n\u001B[1;32m   1375\u001B[0m     \u001B[39mexcept\u001B[39;00m \u001B[39mConnectionError\u001B[39;00m:\n\u001B[1;32m   1376\u001B[0m         \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mclose()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:318\u001B[0m, in \u001B[0;36mHTTPResponse.begin\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    316\u001B[0m \u001B[39m# read until we get a non-100 response\u001B[39;00m\n\u001B[1;32m    317\u001B[0m \u001B[39mwhile\u001B[39;00m \u001B[39mTrue\u001B[39;00m:\n\u001B[0;32m--> 318\u001B[0m     version, status, reason \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_read_status()\n\u001B[1;32m    319\u001B[0m     \u001B[39mif\u001B[39;00m status \u001B[39m!=\u001B[39m CONTINUE:\n\u001B[1;32m    320\u001B[0m         \u001B[39mbreak\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:279\u001B[0m, in \u001B[0;36mHTTPResponse._read_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    278\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39m_read_status\u001B[39m(\u001B[39mself\u001B[39m):\n\u001B[0;32m--> 279\u001B[0m     line \u001B[39m=\u001B[39m \u001B[39mstr\u001B[39m(\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mfp\u001B[39m.\u001B[39;49mreadline(_MAXLINE \u001B[39m+\u001B[39;49m \u001B[39m1\u001B[39;49m), \u001B[39m\"\u001B[39m\u001B[39miso-8859-1\u001B[39m\u001B[39m\"\u001B[39m)\n\u001B[1;32m    280\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39mlen\u001B[39m(line) \u001B[39m>\u001B[39m _MAXLINE:\n\u001B[1;32m    281\u001B[0m         \u001B[39mraise\u001B[39;00m LineTooLong(\u001B[39m\"\u001B[39m\u001B[39mstatus line\u001B[39m\u001B[39m\"\u001B[39m)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socket.py:705\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    703\u001B[0m \u001B[39mwhile\u001B[39;00m \u001B[39mTrue\u001B[39;00m:\n\u001B[1;32m    704\u001B[0m     \u001B[39mtry\u001B[39;00m:\n\u001B[0;32m--> 705\u001B[0m         \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_sock\u001B[39m.\u001B[39;49mrecv_into(b)\n\u001B[1;32m    706\u001B[0m     \u001B[39mexcept\u001B[39;00m timeout:\n\u001B[1;32m    707\u001B[0m         \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_timeout_occurred \u001B[39m=\u001B[39m \u001B[39mTrue\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1274\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[0;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[1;32m   1270\u001B[0m     \u001B[39mif\u001B[39;00m flags \u001B[39m!=\u001B[39m \u001B[39m0\u001B[39m:\n\u001B[1;32m   1271\u001B[0m         \u001B[39mraise\u001B[39;00m \u001B[39mValueError\u001B[39;00m(\n\u001B[1;32m   1272\u001B[0m           \u001B[39m\"\u001B[39m\u001B[39mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[39m%s\u001B[39;00m\u001B[39m\"\u001B[39m \u001B[39m%\u001B[39m\n\u001B[1;32m   1273\u001B[0m           \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m\u001B[39m__class__\u001B[39m)\n\u001B[0;32m-> 1274\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mread(nbytes, buffer)\n\u001B[1;32m   1275\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[1;32m   1276\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39msuper\u001B[39m()\u001B[39m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1130\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[0;34m(self, len, buffer)\u001B[0m\n\u001B[1;32m   1128\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[1;32m   1129\u001B[0m     \u001B[39mif\u001B[39;00m buffer \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[0;32m-> 1130\u001B[0m         \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_sslobj\u001B[39m.\u001B[39;49mread(\u001B[39mlen\u001B[39;49m, buffer)\n\u001B[1;32m   1131\u001B[0m     \u001B[39melse\u001B[39;00m:\n\u001B[1;32m   1132\u001B[0m         \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_sslobj\u001B[39m.\u001B[39mread(\u001B[39mlen\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# GPT model herel \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from gpt import GPT\n",
    "\n",
    "gpt_model = GPT()\n",
    "\n",
    "#gpt_model.answer_question(question='What is the most important thing I need to know about your privacy statement?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT query function here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7865\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "import random\n",
    "\n",
    "# load the pre-trained intent analysis model\n",
    "# nlp = spacy.load(\"en_trf_bertbaseuncased_lg\")\n",
    "\n",
    "response_map = {\n",
    "    (\"security\", \"security_relating_to\"): [\"Our security measures include...\", \"We take security very seriously and have implemented...\"],\n",
    "    (\"security\", \"security_concerns\"): [\"We understand your security concerns and have taken steps to address them.\", \"You can trust that your information is safe with us.\"],\n",
    "    (\"information\", \"information_about\"): [\"Our store offers a variety of products, including...\", \"We also have a rewards program that allows you to earn points on your purchases.\"],\n",
    "    (\"information\", \"information_schedule\"): [\"We are open from 9am to 10pm, 7 days a week.\", \"Our business hours are 9am to 5pm, Monday to Friday.\"],\n",
    "    (\"help\", \"help_with_finding\"): [\"Here are some hotels near the airport:...\", \"I can help you find a hotel that meets your needs.\"],\n",
    "    (\"help\", \"help_with_booking\"): [\"You can book a room on our website or by calling our reservation hotline.\", \"We also offer a loyalty program that gives you discounts on future bookings.\"],\n",
    "    (\"information\", \"ordering\"): [\"You can place an order on our website or by calling our order hotline.\", \"We also offer a loyalty program that gives you discounts on future orders.\"]\n",
    "}\n",
    "\n",
    "with gr.Blocks(css=\"./chat/chat.css\") as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def user(user_message, history):\n",
    "        return \"\", history + [[user_message, None]]\n",
    "\n",
    "    def bot(history):\n",
    "        user_message = history[-1][0]\n",
    "\n",
    "        intent = predict_intent(text=user_message)\n",
    "        print('intent:', intent)\n",
    "        # Random choice randomly chooses one of the options that matches the intent\n",
    "     \n",
    "        # generating a response with GPT if the main intent was 'privacy_policy' or 'legal_statement'\n",
    "        use_gpt = intent[0] == 'privacy_policy' or intent[0] == 'legal_statement'\n",
    "\n",
    "        response = gpt_model.answer_question(question=user_message) if use_gpt else 'No idea, bitch'\n",
    "\n",
    "        # response = random.choice(response_map[intent])\n",
    "        history[-1][1] = response\n",
    "        # The sleep is to simulate a more natural conversation\n",
    "        time.sleep(1)\n",
    "        return history\n",
    "\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "        bot, chatbot, chatbot\n",
    "    )\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
